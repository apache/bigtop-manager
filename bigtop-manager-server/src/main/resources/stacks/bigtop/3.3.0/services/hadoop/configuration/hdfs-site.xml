<?xml version="1.0"?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~ or more contributor license agreements.  See the NOTICE file
  ~ distributed with this work for additional information
  ~ regarding copyright ownership.  The ASF licenses this file
  ~ to you under the Apache License, Version 2.0 (the
  ~ "License"); you may not use this file except in compliance
  ~ with the License.  You may obtain a copy of the License at
  ~
  ~   http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing,
  ~ software distributed under the License is distributed on an
  ~ "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  ~ KIND, either express or implied.  See the License for the
  ~ specific language governing permissions and limitations
  ~ under the License.
-->

<configuration>
    <!-- file system properties -->
    <property>
        <name>dfs.namenode.name.dir</name>
        <!-- cluster variant -->
        <value>/hadoop/dfs/name</value>
        <description>Determines where on the local filesystem the DFS name node
            should store the name table. If this is a comma-delimited list
            of directories then the name table is replicated in all of the
            directories, for redundancy.
        </description>
    </property>
    <property>
        <name>dfs.datanode.failed.volumes.tolerated</name>
        <value>0</value>
        <description>Number of failed disks a DataNode would tolerate before it stops offering service</description>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/hadoop/dfs/data</value>
        <description>Determines where on the local filesystem an DFS data node
            should store its blocks. If this is a comma-delimited
            list of directories, then data will be stored in all named
            directories, typically on different devices.
            Directories that do not exist are ignored.
        </description>
    </property>
    <property>
        <name>dfs.hosts.exclude</name>
        <value>/etc/hadoop/conf/dfs.exclude</value>
        <description>Names a file that contains a list of hosts that are
            not permitted to connect to the namenode. The full pathname of the
            file must be specified. If the value is empty, no hosts are
            excluded.
        </description>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>/hadoop/dfs/namesecondary</value>
        <description>Determines where on the local filesystem the DFS secondary
            name node should store the temporary images to merge.
            If this is a comma-delimited list of directories then the image is
            replicated in all of the directories for redundancy.
        </description>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.edits.dir</name>
        <value>${dfs.namenode.checkpoint.dir}</value>
        <description>Determines where on the local filesystem the DFS secondary
            name node should store the temporary edits to merge.
            If this is a comma-delimited list of directories then the edits are
            replicated in all of the directories for redundancy.
            Default value is same as dfs.namenode.checkpoint.dir
        </description>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.period</name>
        <value>3600</value>
        <description>The number of seconds between two periodic checkpoints.</description>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.txns</name>
        <value>1000000</value>
        <description>The Secondary NameNode or CheckpointNode will create a checkpoint
            of the namespace every 'dfs.namenode.checkpoint.txns' transactions,
            regardless of whether 'dfs.namenode.checkpoint.period' has expired.
        </description>
    </property>
    <property>
        <name>hadoop.proxyuser.hive.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>dfs.replication.max</name>
        <value>512</value>
        <description>Maximal block replication.
        </description>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
        <description>Default block replication.</description>
    </property>
    <property>
        <name>dfs.heartbeat.interval</name>
        <value>3</value>
        <description>Determines datanode heartbeat interval in seconds.</description>
    </property>
    <property>
        <name>dfs.namenode.safemode.threshold-pct</name>
        <value>0.999f</value>
        <description>
            Specifies the percentage of blocks that should satisfy
            the minimal replication requirement defined by dfs.namenode.replication.min.
            Values less than or equal to 0 mean not to start in safe mode.
            Values greater than 1 will make safe mode permanent.
        </description>
    </property>
    <property>
        <name>dfs.datanode.balance.bandwidthPerSec</name>
        <value>100m</value>
        <description>
            Specifies the maximum amount of bandwidth that each datanode
            can utilize for the balancing purpose in term of
            the number of bytes per second.
        </description>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>0.0.0.0:9868</value>
        <description>The secondary namenode http server address and port.</description>
    </property>
    <property>
        <name>dfs.namenode.secondary.https-address</name>
        <value>0.0.0.0:9869</value>
        <description>The secondary namenode HTTPS server address and port.</description>
    </property>
    <property>
        <name>dfs.datanode.address</name>
        <value>0.0.0.0:9866</value>
        <description>The datanode server address and port for data transfer.</description>
    </property>
    <property>
        <name>dfs.datanode.http.address</name>
        <value>0.0.0.0:9864</value>
        <description>The datanode http server address and port.</description>
    </property>
    <property>
        <name>dfs.datanode.ipc.address</name>
        <value>0.0.0.0:9867</value>
        <description>The datanode ipc server address and port.</description>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>0.0.0.0:9870</value>
        <description>The address and the base port where the dfs namenode web ui will listen on.</description>
    </property>
    <!--https-->
    <property>
        <name>dfs.datanode.https.address</name>
        <value>0.0.0.0:9865</value>
        <description>The datanode secure http server address and port.</description>
    </property>
    <property>
        <name>dfs.namenode.https-address</name>
        <value>0.0.0.0:9871</value>
        <description>The namenode secure http server address and port.</description>
    </property>
    <!--https-->
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
        <description>The default block size for new files.</description>
    </property>
    <property>
        <name>dfs.namenode.rpc-address</name>
        <value>0.0.0.0:8020</value>
        <description>RPC address that handles all clients requests.</description>
    </property>
    <property>
        <name>dfs.datanode.du.reserved</name>
        <!-- cluster variant -->
        <value>1073741824</value>
        <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
        </description>
    </property>
    <property>
        <name>dfs.blockreport.initialDelay</name>
        <value>0</value>
        <description>Delay for first block report in seconds.</description>
    </property>
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>4096</value>
        <description>Specifies the maximum number of threads to use for transferring data in and out of the datanode.
        </description>
    </property>
    <!-- Permissions configuration -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
        <description>
            If "true", enable permission checking in HDFS.
            If "false", permission checking is turned off,
            but all other behavior is unchanged.
            Switching from one parameter value to the other does not change the mode,
            owner or group of files or directories.
        </description>
    </property>
    <property>
        <name>dfs.permissions.superusergroup</name>
        <value>hadoop</value>
        <description>The name of the group of super-users.</description>
    </property>
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>100</value>
        <description>Added to grow Queue size so that more client connections are allowed</description>
    </property>
    <property>
        <name>dfs.block.access.token.enable</name>
        <value>true</value>
        <description>
            If "true", access tokens are used as capabilities for accessing datanodes.
            If "false", no access tokens are checked on accessing datanodes.
        </description>
    </property>
    <property>
        <name>dfs.datanode.data.dir.perm</name>
        <value>700</value>
        <description>The permissions that should be there on dfs.datanode.data.dir
            directories. The datanode will not come up if the permissions are
            different on existing dfs.datanode.data.dir directories. If the directories
            don't exist, they will be created with this permission.
        </description>
    </property>
    <property>
        <name>dfs.namenode.accesstime.precision</name>
        <value>0</value>
        <description>The access time for HDFS file is precise up to this value.
            The default value is 1 hour. Setting a value of 0 disables
            access times for HDFS.
        </description>
    </property>
    <property>
        <name>dfs.cluster.administrators</name>
        <value>hdfs</value>
        <description>ACL for the admins, this configuration is used to control who can access the default servlets in
            the namenode, etc. The value should be a comma separated list of users and groups. The user list comes first
            and is separated by a space followed by the group list, e.g. "user1,user2 group1,group2". Both users and
            groups are optional, so "user1", " group1", "", "user1 group1", "user1,user2 group1,group2" are all valid
            (note the leading space in " group1"). '*' grants access to all users and groups, e.g. '*', '* ' and ' *'
            are all valid.
        </description>
    </property>
    <property>
        <name>dfs.namenode.avoid.read.stale.datanode</name>
        <value>true</value>
        <description>
            Indicate whether or not to avoid reading from stale datanodes whose
            heartbeat messages have not been received by the namenode for more than a
            specified time interval.
        </description>
    </property>
    <property>
        <name>dfs.namenode.avoid.write.stale.datanode</name>
        <value>true</value>
        <description>
            Indicate whether or not to avoid writing to stale datanodes whose
            heartbeat messages have not been received by the namenode for more than a
            specified time interval.
        </description>
    </property>
    <property>
        <name>dfs.namenode.write.stale.datanode.ratio</name>
        <value>1.0f</value>
        <description>When the ratio of number stale datanodes to total datanodes marked is greater
            than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.
        </description>
    </property>
    <property>
        <name>dfs.namenode.stale.datanode.interval</name>
        <value>30000</value>
        <description>Datanode is stale after not getting a heartbeat in this interval in ms</description>
    </property>
    <property>
        <name>dfs.journalnode.http-address</name>
        <value>0.0.0.0:8480</value>
        <description>The address and port the JournalNode web UI listens on.
            If the port is 0 then the server will start on a free port.
        </description>
    </property>
    <property>
        <name>dfs.journalnode.https-address</name>
        <value>0.0.0.0:8481</value>
        <description>The address and port the JournalNode HTTPS server listens on.
            If the port is 0 then the server will start on a free port.
        </description>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/hadoop/dfs/journalnode</value>
        <description>The path where the JournalNode daemon will store its local state.</description>
    </property>
    <!-- HDFS Short-Circuit Local Reads -->
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>true</value>
        <description>
            This configuration parameter turns on short-circuit local reads.
        </description>
    </property>
    <property>
        <name>dfs.domain.socket.path</name>
        <value>/var/run/hadoop/dn._PORT</value>
        <description>
            This is a path to a UNIX domain socket that will be used for communication between the DataNode and local
            HDFS clients.
            If the string "_PORT" is present in this path, it will be replaced by the TCP port of the DataNode.
        </description>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit.streams.cache.size</name>
        <value>4096</value>
        <description>
            The DFSClient maintains a cache of recently opened file descriptors. This
            parameter controls the size of that cache. Setting this higher will use
            more file descriptors, but potentially provide better performance on
            workloads involving lots of seeks.
        </description>
    </property>
    <property>
        <name>dfs.namenode.name.dir.restore</name>
        <value>true</value>
        <description>Set to true to enable NameNode to attempt recovering a previously failed dfs.namenode.name.dir.
            When enabled, a recovery of any failed directory is attempted during checkpoint.
        </description>
    </property>
    <property>
        <name>dfs.http.policy</name>
        <value>HTTP_ONLY</value>
        <description>
            Decide if HTTPS(SSL) is supported on HDFS This configures the HTTP endpoint for HDFS daemons:
            The following values are supported: - HTTP_ONLY : Service is provided only on http - HTTPS_ONLY :
            Service is provided only on https - HTTP_AND_HTTPS : Service is provided both on http and https
        </description>
    </property>
</configuration>

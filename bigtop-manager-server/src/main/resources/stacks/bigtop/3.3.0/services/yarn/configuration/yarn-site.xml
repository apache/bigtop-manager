<?xml version="1.0"?>
<!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~ or more contributor license agreements.  See the NOTICE file
  ~ distributed with this work for additional information
  ~ regarding copyright ownership.  The ASF licenses this file
  ~ to you under the Apache License, Version 2.0 (the
  ~ "License"); you may not use this file except in compliance
  ~ with the License.  You may obtain a copy of the License at
  ~
  ~   http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing,
  ~ software distributed under the License is distributed on an
  ~ "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  ~ KIND, either express or implied.  See the License for the
  ~ specific language governing permissions and limitations
  ~ under the License.
-->
<configuration>
    <!-- ResourceManager -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>localhost</value>
        <description>The hostname of the RM.</description>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>0.0.0.0:8031</value>
        <description> The address of ResourceManager. </description>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>0.0.0.0:8030</value>
        <description>The address of the scheduler interface.</description>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>0.0.0.0:8032</value>
        <description>The address of the applications manager interface in the RM.</description>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>0.0.0.0:8033</value>
        <description>The address of the RM admin interface.</description>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
        <description>The class to use as the resource scheduler.</description>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
        <description>
            The minimum allocation for every container request at the RM,
            in MBs. Memory requests lower than this won't take effect,
            and the specified value will get allocated at minimum.
        </description>
        <display-name>Minimum Container Size (Memory)</display-name>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>5120</value>
        <description>
            The maximum allocation for every container request at the RM,
            in MBs. Memory requests higher than this won't take effect,
            and will get capped to this value.
        </description>
        <display-name>Maximum Container Size (Memory)</display-name>
    </property>
    <property>
        <name>yarn.acl.enable</name>
        <value>false</value>
        <description>Are acls enabled.</description>
    </property>
    <property>
        <name>yarn.admin.acl</name>
        <value />
        <description>ACL of who can be admin of the YARN cluster.</description>
    </property>
    <!-- NodeManager -->
    <property>
        <name>yarn.nodemanager.address</name>
        <value>0.0.0.0:45454</value>
        <description>The address of the container manager in the NM.</description>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>5120</value>
        <description>Amount of physical memory, in MB, that can be allocated for containers.</description>
        <display-name>Memory allocated for all YARN containers on a node</display-name>
    </property>
    <property>
        <name>yarn.application.classpath</name>
        <value>${hadoop_conf_dir!},${hadoop_home!}/*,${hadoop_home!}/lib/*,${hadoop_hdfs_home!}/*,${hadoop_hdfs_home!}/lib/*,${hadoop_yarn_home!}/*,${hadoop_yarn_home!}/lib/*</value>
        <description>Classpath for typical applications.</description>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
        <description>
            Ratio between virtual memory to physical memory when
            setting memory limits for containers. Container allocations are
            expressed in terms of physical memory, and virtual memory usage
            is allowed to exceed this allocation by this ratio.
        </description>
        <display-name>Virtual Memory Ratio</display-name>
    </property>
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor</value>
        <description>ContainerExecutor for launching containers</description>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.group</name>
        <value>hadoop</value>
        <description>Unix group of the NodeManager</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>
            Auxiliary services of NodeManager.
            A valid service name should only contain a-zA-Z0-9_ and can not start with numbers
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        <description>The auxiliary service class to use</description>
    </property>
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <display-name>YARN NodeManager Log directories</display-name>
        <value>/hadoop/yarn/log</value>
        <description>
            Where to store container logs. An application's localized log directory
            will be found in ${yarn.nodemanager.log-dirs}/application_${appid}.
            Individual containers' log directories will be below this, in directories
            named container_{$contid}. Each container directory will contain the files
            stderr, stdin, and syslog generated by that container.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <display-name>YARN NodeManager Local directories</display-name>
        <value>/hadoop/yarn/local</value>
        <description>
            List of directories to store localized files in. An
            application's localized file directory will be found in:
            ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}.
            Individual containers' work directories, called container_${contid}, will
            be subdirectories of this.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.container-monitor.interval-ms</name>
        <value>3000</value>
        <description>
            The interval, in milliseconds, for which the node manager
            waits between two cycles of monitoring its containers' memory usage.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.health-checker.interval-ms</name>
        <value>135000</value>
        <description>Frequency of running node health script.</description>
    </property>
    <property>
        <name>yarn.nodemanager.health-checker.script.timeout-ms</name>
        <value>60000</value>
        <description>Script time out period.</description>
    </property>
    <property>
        <name>yarn.nodemanager.log.retain-seconds</name>
        <value>604800</value>
        <description>
            Time in seconds to retain user logs. Only applicable if log aggregation is disabled.
        </description>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
        <description>Whether to enable log aggregation.</description>
        <display-name>Enable Log Aggregation</display-name>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <display-name>YARN NodeManager Remote App Log directory</display-name>
        <value>/app-logs</value>
        <description>Location to aggregate logs to.</description>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
        <value>logs</value>
        <description>
            The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.log-aggregation.compression-type</name>
        <value>gz</value>
        <description>
            T-file compression types used to compress aggregated logs.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.delete.debug-delay-sec</name>
        <value>0</value>
        <description>
            Number of seconds after an application finishes before the nodemanager's
            DeletionService will delete the application's localized file directory
            and log directory.

            To diagnose Yarn application problems, set this property's value large
            enough (for example, to 600 = 10 minutes) to permit examination of these
            directories. After changing the property's value, you must restart the
            nodemanager in order for it to have an effect.

            The roots of Yarn applications' work directories is configurable with
            the yarn.nodemanager.local-dirs property (see below), and the roots
            of the Yarn applications' log directories is configurable with the
            yarn.nodemanager.log-dirs property (see also below).
        </description>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>2592000</value>
        <description>
            How long to keep aggregation logs before deleting them. -1 disables.
            Be careful set this too small and you will spam the name node.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.admin-env</name>
        <value>MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX</value>
        <description>
            Environment variables that should be forwarded from the NodeManager's
            environment to the container's.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
        <value>0.25</value>
        <description>
            The minimum fraction of number of disks to be healthy for the nodemanager
            to launch new containers. This correspond to both
            yarn-nodemanager.local-dirs and yarn.nodemanager.log-dirs. i.e.
            If there are less number of healthy local-dirs (or log-dirs) available,
            then new containers will not be launched on this node.
        </description>
    </property>
    <property>
        <name>yarn.resourcemanager.am.max-attempts</name>
        <value>2</value>
        <description>
            The maximum number of application attempts. It's a global
            setting for all application masters. Each application master can specify
            its individual maximum number of application attempts via the API, but the
            individual number cannot be more than the global upper bound. If it is,
            the resourcemanager will override it. The default number is set to 2, to
            allow at least one retry for AM.
        </description>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>0.0.0.0:8088</value>
        <description>
            The address of the RM web application.
        </description>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.https.address</name>
        <value>0.0.0.0:8090</value>
        <description>
            The https address of the RM web application.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
        <description>
            Whether virtual memory limits will be enforced for containers.
        </description>
    </property>
    <property>
        <name>yarn.log.server.url</name>
        <value>http://localhost:19888/jobhistory/logs</value>
        <description>
            URI for the HistoryServer's log resource
        </description>
    </property>
    <property>
        <name>yarn.resourcemanager.nodes.exclude-path</name>
        <value>/etc/hadoop/conf/yarn.exclude</value>
        <description>
            Names a file that contains a list of hosts that are
            not permitted to connect to the resource manager. The full pathname of the
            file must be specified. If the value is empty, no hosts are
            excluded.
        </description>
    </property>
    <property>
        <name>manage.include.files</name>
        <value>false</value>
        <description>If true this will manage include file if
            yarn.resourcemanager.nodes.include-path is configured.
        </description>
    </property>
    <property>
        <name>yarn.http.policy</name>
        <value>HTTP_ONLY</value>
        <description>
            This configures the HTTP endpoint for Yarn Daemons.The following values are supported: -
            HTTP_ONLY : Service is provided only on http - HTTPS_ONLY : Service is provided only on https
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.enabled</name>
        <value>true</value>
        <description>
            Indicate to clients whether timeline service is enabled or not.
            If enabled, clients will put entities and events to the timeline server.
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.generic-application-history.store-class</name>
        <value>org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore</value>
        <description>
            Store class name for history store, defaulting to file system store
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.leveldb-timeline-store.path</name>
        <value>/var/log/hadoop-yarn/timeline</value>
        <description>
            Store file name for leveldb timeline store
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.webapp.address</name>
        <value>0.0.0.0:8188</value>
        <description>
            The http address of the timeline service web application.
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.webapp.https.address</name>
        <value>0.0.0.0:8190</value>
        <description>
            The http address of the timeline service web application.
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.address</name>
        <value>0.0.0.0:10200</value>
        <description>
            This is default address for the timeline server to start
            the RPC server.
        </description>
    </property>
    <property>
        <description>Enable age off of timeline store data.</description>
        <name>yarn.timeline-service.ttl-enable</name>
        <value>true</value>
    </property>
    <property>
        <description>Time to live for timeline store data in milliseconds.</description>
        <name>yarn.timeline-service.ttl-ms</name>
        <value>2678400000</value>
    </property>
    <property>
        <name>yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms</name>
        <value>300000</value>
        <description>Length of time to wait between deletion cycles of leveldb timeline store in milliseconds.</description>
    </property>
    <property>
        <name>yarn.timeline-service.recovery.enabled</name>
        <value>true</value>
        <description>
            Enable timeline server to recover state after starting. If
            true, then yarn.timeline-service.state-store-class must be specified.
        </description>
    </property>
    <property>
        <name>yarn.acl.enable</name>
        <value>false</value>
        <description>Are acls enabled.</description>
    </property>
    <property>
        <name>yarn.authorization-provider</name>
        <description>Yarn authorization provider class.</description>
    </property>
    <property>
        <name>yarn.admin.acl</name>
        <value>yarn</value>
        <description>ACL of who can be admin of the YARN cluster.</description>
    </property>
    <!--ats
    v1.5 properties-->
    <property>
        <name>yarn.timeline-service.store-class</name>
        <value>org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore</value>
        <description>Main storage class for YARN timeline server.</description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.active-dir</name>
        <value>/ats/active/</value>
        <description>DFS path to store active application&#x2019;s timeline data</description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.done-dir</name>
        <value>/ats/done/</value>
        <description>DFS path to store done application&#x2019;s timeline data</description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes</name>
        <value />
        <description>
            Plugins that can translate a timeline entity read request into a list of timeline cache ids, separated by commas.
        </description>
    </property>
    <!-- advanced ats v1.5 properties-->
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.summary-store</name>
        <!-- Use rolling leveldb, advanced -->
        <value>org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore</value>
        <description>Summary storage for ATS v1.5</description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.scan-interval-seconds</name>
        <!-- Default is 60 seconds, advanced -->
        <value>60</value>
        <description>
            Scan interval for ATS v1.5 entity group file system storage reader.This
            value controls how frequent the reader will scan the HDFS active directory
            for application status.
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds</name>
        <!-- 3600 is default, advanced -->
        <value>3600</value>
        <description>
            Scan interval for ATS v1.5 entity group file system storage cleaner.This
            value controls how frequent the reader will scan the HDFS done directory
            for stale application data.
        </description>
    </property>
    <property>
        <name>yarn.timeline-service.entity-group-fs-store.retain-seconds</name>
        <!-- 7 days is default, advanced -->
        <value>604800</value>
        <description>
            How long the ATS v1.5 entity group file system storage will keep an
            application's data in the done directory.
        </description>
    </property>
    <property>
        <name>yarn.log.server.web-service.url</name>
        <value>http://localhost:8188/ws/v1/applicationhistory</value>
        <description>Log Server Web Service URL.</description>
    </property>
</configuration>